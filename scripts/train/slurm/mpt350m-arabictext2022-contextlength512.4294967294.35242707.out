Training using config: 
variables:
  data_local: /ibex/ai/project/c2254/itanh0b/llm-foundry/tokenized_datasets/ArabicText2022_bert_v32000_512
  data_remote: null
  max_seq_len: 512
  mlm_probability: 0.3
  run_name: test-bert
run_name: ${variables.run_name}
max_seq_len: ${variables.max_seq_len}
model:
  name: mosaic_bert
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
  model_config:
    vocab_size: 32000
    num_attention_heads: 12
    num_hidden_layers: 12
    attention_probs_dropout_prob: 0.0
tokenizer:
  name: /ibex/ai/project/c2254/itanh0b/tokenizers/bert-base-uncased-v32000-arabic
  kwargs:
    model_max_length: ${variables.max_seq_len}
train_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: train
    max_seq_len: ${variables.max_seq_len}
    shuffle: true
    mlm_probability: ${variables.mlm_probability}
  drop_last: true
  num_workers: 6
eval_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: val
    max_seq_len: ${variables.max_seq_len}
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 6
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.01
optimizer:
  name: decoupled_adamw
  lr: 0.0005
  betas:
  - 0.9
  - 0.96
  eps: 1.0e-06
  weight_decay: 5.0e-05
algorithms:
  low_precision_layernorm: {}
max_duration: 20ep
eval_interval: 1ep
global_train_batch_size: 4096
seed: 17
device_eval_batch_size: 256
device_train_microbatch_size: 64
precision: amp_bf16
progress_bar: true
log_to_console: true
console_log_interval: 100ba
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: llm-foundry-scripts_train-zehua-bert
save_interval: 2000ba
save_num_checkpoints_to_keep: 10
save_folder: ./{run_name}/checkpoints

Initializing model...
n_params=1.3853e+08
Building train loader...
Building eval loader...
Logging config...
variables:
  data_local: /ibex/ai/project/c2254/itanh0b/llm-foundry/tokenized_datasets/ArabicText2022_bert_v32000_512
  data_remote: null
  max_seq_len: 512
  mlm_probability: 0.3
  run_name: test-bert
run_name: test-bert
max_seq_len: 512
model:
  name: mosaic_bert
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
  model_config:
    vocab_size: 32000
    num_attention_heads: 12
    num_hidden_layers: 12
    attention_probs_dropout_prob: 0.0
tokenizer:
  name: /ibex/ai/project/c2254/itanh0b/tokenizers/bert-base-uncased-v32000-arabic
  kwargs:
    model_max_length: 512
train_loader:
  name: text
  dataset:
    local: /ibex/ai/project/c2254/itanh0b/llm-foundry/tokenized_datasets/ArabicText2022_bert_v32000_512
    remote: null
    split: train
    max_seq_len: 512
    shuffle: true
    mlm_probability: 0.3
  drop_last: true
  num_workers: 6
eval_loader:
  name: text
  dataset:
    local: /ibex/ai/project/c2254/itanh0b/llm-foundry/tokenized_datasets/ArabicText2022_bert_v32000_512
    remote: null
    split: val
    max_seq_len: 512
    shuffle: false
    mlm_probability: 0.15
  drop_last: false
  num_workers: 6
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur
  alpha_f: 0.01
optimizer:
  name: decoupled_adamw
  lr: 0.0005
  betas:
  - 0.9
  - 0.96
  eps: 1.0e-06
  weight_decay: 5.0e-05
algorithms:
  low_precision_layernorm: {}
max_duration: 20ep
eval_interval: 1ep
global_train_batch_size: 4096
seed: 17
device_eval_batch_size: 256
device_train_microbatch_size: 64
precision: amp_bf16
progress_bar: true
log_to_console: true
console_log_interval: 100ba
callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}
loggers:
  wandb:
    project: llm-foundry-scripts_train-zehua-bert
save_interval: 2000ba
save_num_checkpoints_to_keep: 10
save_folder: ./{run_name}/checkpoints
n_gpus: 4
device_train_batch_size: 1024
device_train_grad_accum: 16
merge: true

Starting training...
