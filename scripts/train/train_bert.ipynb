{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 MosaicML Examples authors\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional, cast\n",
    "\n",
    "import llmfoundry.data.text_data as text_data_module\n",
    "from llmfoundry.models.mosaicbert.mosaic_bert import create_mosaic_bert_mlm\n",
    "from llmfoundry.models.mosaicbert.hf_bert import create_hf_bert_mlm\n",
    "from llmfoundry.optim.scheduler import InverseSquareRootWithWarmupScheduler\n",
    "from llmfoundry.data.dataloader import build_dataloader\n",
    "\n",
    "from composer import Trainer, algorithms\n",
    "from composer.callbacks import (LRMonitor, MemoryMonitor,\n",
    "                                OptimizerMonitor, RuntimeEstimator,\n",
    "                                SpeedMonitor)\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.optim import DecoupledAdamW\n",
    "from composer.optim.scheduler import (ConstantWithWarmupScheduler,\n",
    "                                      CosineAnnealingWithWarmupScheduler,\n",
    "                                      LinearWithWarmupScheduler)\n",
    "from composer.utils import dist, reproducibility\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from llmfoundry.utils.config_utils import (\n",
    "    TRAIN_CONFIG_KEYS,\n",
    "    TrainConfig,\n",
    "    log_config,\n",
    "    make_dataclass_and_log_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_batch_size_info(cfg: DictConfig):\n",
    "    global_batch_size, device_microbatch_size = cfg.global_train_batch_size, cfg.device_train_microbatch_size\n",
    "    if global_batch_size % dist.get_world_size() != 0:\n",
    "        raise ValueError(\n",
    "            f'Global batch size {global_batch_size} is not divisible by {dist.get_world_size()} '\n",
    "            'as a result, the batch size would be truncated, please adjust `global_batch_size` '\n",
    "            f'to be divisible by world size, {dist.get_world_size()}.')\n",
    "    device_train_batch_size = global_batch_size // dist.get_world_size()\n",
    "    if isinstance(device_microbatch_size, int):\n",
    "        if device_microbatch_size > device_train_batch_size:\n",
    "            print(\n",
    "                f'WARNING: device_train_microbatch_size > device_train_batch_size, '\n",
    "                f'will be reduced from {device_microbatch_size} -> {device_train_batch_size}.'\n",
    "            )\n",
    "            device_microbatch_size = device_train_batch_size\n",
    "    cfg.n_gpus = dist.get_world_size()\n",
    "    cfg.device_train_batch_size = device_train_batch_size\n",
    "    cfg.device_train_microbatch_size = device_microbatch_size\n",
    "    # Safely set `device_eval_batch_size` if not provided by user\n",
    "    if 'device_eval_batch_size' not in cfg:\n",
    "        if cfg.device_train_microbatch_size == 'auto':\n",
    "            cfg.device_eval_batch_size = 1\n",
    "        else:\n",
    "            cfg.device_eval_batch_size = cfg.device_train_microbatch_size\n",
    "    return cfg\n",
    "\n",
    "def build_algorithm(name, kwargs):\n",
    "    if name == 'gradient_clipping':\n",
    "        return algorithms.GradientClipping(**kwargs)\n",
    "    elif name == 'alibi':\n",
    "        return algorithms.Alibi(**kwargs)\n",
    "    elif name == 'fused_layernorm':\n",
    "        return algorithms.FusedLayerNorm(**kwargs)\n",
    "    elif name == 'gated_linear_units':\n",
    "        return algorithms.GatedLinearUnits(**kwargs)\n",
    "    elif name == 'low_precision_layernorm':\n",
    "        return algorithms.LowPrecisionLayerNorm(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build algorithm: {name}')\n",
    "\n",
    "\n",
    "def build_callback(name, kwargs):\n",
    "    if name == 'lr_monitor':\n",
    "        return LRMonitor()\n",
    "    elif name == 'memory_monitor':\n",
    "        return MemoryMonitor()\n",
    "    elif name == 'speed_monitor':\n",
    "        return SpeedMonitor(window_size=kwargs.get('window_size', 1),\n",
    "                            gpu_flops_available=kwargs.get(\n",
    "                                'gpu_flops_available', None))\n",
    "    elif name == 'runtime_estimator':\n",
    "        return RuntimeEstimator()\n",
    "    elif name == 'optimizer_monitor':\n",
    "        return OptimizerMonitor(log_optimizer_metrics=kwargs.get(\n",
    "            'log_optimizer_metrics', True),)\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build callback: {name}')\n",
    "\n",
    "\n",
    "def build_logger(name, kwargs):\n",
    "    if name == 'wandb':\n",
    "        return WandBLogger(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build logger: {name}')\n",
    "\n",
    "\n",
    "def build_scheduler(cfg):\n",
    "    if cfg.name == 'constant_with_warmup':\n",
    "        return ConstantWithWarmupScheduler(t_warmup=cfg.t_warmup)\n",
    "    elif cfg.name == 'cosine_with_warmup':\n",
    "        return CosineAnnealingWithWarmupScheduler(t_warmup=cfg.t_warmup,\n",
    "                                                  alpha_f=cfg.alpha_f)\n",
    "    elif cfg.name == 'linear_decay_with_warmup':\n",
    "        return LinearWithWarmupScheduler(t_warmup=cfg.t_warmup)\n",
    "    elif cfg.name == 'inv_sqrt_with_warmup':\n",
    "        return InverseSquareRootWithWarmupScheduler(t_warmup=cfg.t_warmup,\n",
    "                                                    t_scale=cfg.t_scale,\n",
    "                                                    t_cooldown=cfg.t_cooldown)\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build scheduler: {cfg.name}')\n",
    "\n",
    "\n",
    "def build_optimizer(cfg, model):\n",
    "    if cfg.name == 'decoupled_adamw':\n",
    "        return DecoupledAdamW(model.parameters(),\n",
    "                              lr=cfg.lr,\n",
    "                              betas=cfg.betas,\n",
    "                              eps=cfg.eps,\n",
    "                              weight_decay=cfg.weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build optimizer: {cfg.name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_dataloader(cfg, tokenizer, device_batch_size):\n",
    "#     if cfg.name == 'text':\n",
    "#         return text_data_module.build_text_dataloader(tokenizer, device_batch_size, cfg,  \n",
    "#                                                       cfg.drop_last, cfg.num_workers)\n",
    "#     else:\n",
    "#         raise ValueError(f'Not sure how to build dataloader with config: {cfg}')\n",
    "\n",
    "\n",
    "def build_model(cfg: DictConfig):\n",
    "    if cfg.name == 'hf_bert':\n",
    "        return create_hf_bert_mlm(\n",
    "            pretrained_model_name=cfg.pretrained_model_name,\n",
    "            use_pretrained=cfg.get('use_pretrained', None),\n",
    "            model_config=cfg.get('model_config', None),\n",
    "            tokenizer_name=cfg.get('tokenizer_name', None),\n",
    "            gradient_checkpointing=cfg.get('gradient_checkpointing', None))\n",
    "    elif cfg.name == 'mosaic_bert':\n",
    "        return create_mosaic_bert_mlm(\n",
    "            pretrained_model_name=cfg.pretrained_model_name,\n",
    "            pretrained_checkpoint=cfg.get('pretrained_checkpoint', None),\n",
    "            model_config=cfg.get('model_config', None),\n",
    "            tokenizer_name=cfg.get('tokenizer_name', None),\n",
    "            gradient_checkpointing=cfg.get('gradient_checkpointing', None))\n",
    "    else:\n",
    "        raise ValueError(f'Not sure how to build model with name={cfg.name}')\n",
    "\n",
    "\n",
    "def main(cfg: DictConfig,\n",
    "         return_trainer: bool = False,\n",
    "         do_train: bool = True) -> Optional[Trainer]:\n",
    "    \n",
    "    print('Training using config: ')\n",
    "    print(om.to_yaml(cfg))\n",
    "    reproducibility.seed_all(cfg.seed)\n",
    "\n",
    "    # Get batch size info\n",
    "    cfg = update_batch_size_info(cfg)\n",
    "\n",
    "    logged_cfg, dataclass_cfg = make_dataclass_and_log_config(\n",
    "        cfg,\n",
    "        TrainConfig,\n",
    "        TRAIN_CONFIG_KEYS,\n",
    "        transforms='all',\n",
    "    )\n",
    "    \n",
    "    # Build Model\n",
    "    print('Initializing model...')\n",
    "    model = build_model(cfg.model)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{n_params=:.4e}')\n",
    "\n",
    "    # Dataloaders\n",
    "    print('Building train loader...')\n",
    "    train_loader = build_dataloader(\n",
    "        dataclass_cfg.train_loader,\n",
    "        model.tokenizer,\n",
    "        cfg.global_train_batch_size // dist.get_world_size(),\n",
    "    )\n",
    "    print('Building eval loader...')\n",
    "\n",
    "    eval_loader = build_dataloader(\n",
    "        dataclass_cfg.eval_loader,\n",
    "        model.tokenizer,\n",
    "        cfg.device_eval_batch_size,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = build_optimizer(cfg.optimizer, model)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = build_scheduler(cfg.scheduler)\n",
    "\n",
    "    # Loggers\n",
    "    loggers = [\n",
    "        build_logger(name, logger_cfg)\n",
    "        for name, logger_cfg in cfg.get('loggers', {}).items()\n",
    "    ]\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        build_callback(name, callback_cfg)\n",
    "        for name, callback_cfg in cfg.get('callbacks', {}).items()\n",
    "    ]\n",
    "\n",
    "    # Algorithms\n",
    "    algorithms = [\n",
    "        build_algorithm(name, algorithm_cfg)\n",
    "        for name, algorithm_cfg in cfg.get('algorithms', {}).items()\n",
    "    ]\n",
    "\n",
    "    if cfg.get('run_name') is None:\n",
    "        cfg.run_name = os.environ.get('COMPOSER_RUN_NAME', 'bert')\n",
    "\n",
    "    # Build the Trainer\n",
    "    trainer = Trainer(\n",
    "        run_name=cfg.run_name,\n",
    "        seed=cfg.seed,\n",
    "        model=model,\n",
    "        algorithms=algorithms,\n",
    "        train_dataloader=train_loader,\n",
    "        eval_dataloader=eval_loader,\n",
    "        train_subset_num_batches=cfg.get('train_subset_num_batches', -1),\n",
    "        eval_subset_num_batches=cfg.get('eval_subset_num_batches', -1),\n",
    "        optimizers=optimizer,\n",
    "        schedulers=scheduler,\n",
    "        max_duration=cfg.max_duration,\n",
    "        eval_interval=cfg.eval_interval,\n",
    "        progress_bar=cfg.progress_bar,\n",
    "        log_to_console=cfg.log_to_console,\n",
    "        console_log_interval=cfg.console_log_interval,\n",
    "        loggers=loggers,\n",
    "        callbacks=callbacks,\n",
    "        precision=cfg.precision,\n",
    "        device=cfg.get('device', None),\n",
    "        device_train_microbatch_size=cfg.get('device_train_microbatch_size',\n",
    "                                             'auto'),\n",
    "        save_folder=cfg.get('save_folder', None),\n",
    "        save_interval=cfg.get('save_interval', '1000ba'),\n",
    "        save_num_checkpoints_to_keep=cfg.get('save_num_checkpoints_to_keep',\n",
    "                                             -1),\n",
    "        save_overwrite=cfg.get('save_overwrite', False),\n",
    "        load_path=cfg.get('load_path', None),\n",
    "        load_weights_only=cfg.get('load_weights_only', False),\n",
    "        python_log_level=cfg.get('python_log_level', None),\n",
    "    )\n",
    "\n",
    "    print('Logging config...')\n",
    "    log_config(logged_cfg)\n",
    "\n",
    "    if do_train:\n",
    "        print('Starting training...')\n",
    "        trainer.fit()\n",
    "\n",
    "    if return_trainer:\n",
    "        return trainer\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    yaml_path, args_list = sys.argv[1], sys.argv[2:]\n",
    "    \n",
    "    with open(yaml_path) as f:\n",
    "        yaml_cfg = om.load(f)\n",
    "    cli_cfg = om.from_cli(args_list)\n",
    "    cfg = om.merge(yaml_cfg, cli_cfg)\n",
    "    cfg = cast(DictConfig, cfg)  # for type checking\n",
    "    main(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
