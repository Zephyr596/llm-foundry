# Note that some of the fields in this template haven't been filled in yet.
# Please resolve any `null` fields before launching!

# Follow the instructions in the README to set up ./my-copy-c4
# Or point data paths to your remote C4 dataset
variables:
  data_local: /ibex/ai/project/c2254/itanh0b/llm-foundry/tokenized_datasets/ArabicText2022_bert_v32000_512
  data_remote: # If blank, files must be present in data_local
  max_seq_len: 512
  mlm_probability: 0.3 # Mosaic BERT should use 30% masking for optimal performance
  # Run Name
  run_name:  test-bert # If left blank, will be read from env var $RUN_NAME

# Run Name
run_name: ${variables.run_name}
max_seq_len: ${variables.max_seq_len}

# Model
model:
  name: mosaic_bert
  pretrained_model_name: bert-base-uncased
  tokenizer_name: bert-base-uncased
  # Mosaic BERT 'base' generally uses the default architecture values for from the Hugging Face BertConfig object
  # Note: if using the pretrained_checkpoint argument to create a model from an existing checkpoint, make sure
  # the model_config settings match the architecture of the existing model
  model_config:
    vocab_size: 32000
    num_attention_heads: 12 # bert-base default
    num_hidden_layers: 12 # bert-base default
    attention_probs_dropout_prob: 0.0 # This must be 0 for Flash Attention with triton to work

tokenizer:
  name: /ibex/ai/project/c2254/itanh0b/tokenizers/bert-base-uncased-v32000-arabic
  kwargs:
    model_max_length: ${variables.max_seq_len}

# Dataloaders
train_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: train
    max_seq_len: ${variables.max_seq_len}
    shuffle: true
    mlm_probability: ${variables.mlm_probability}
  drop_last: true
  num_workers: 6

eval_loader:
  name: text
  dataset:
    local: ${variables.data_local}
    remote: ${variables.data_remote}
    split: val
    max_seq_len: ${variables.max_seq_len}
    shuffle: false
    mlm_probability: 0.15 # We always evaluate at 15% masking for consistent comparison
  drop_last: false
  num_workers: 6

# Optimization
scheduler:
  name: linear_decay_with_warmup
  t_warmup: 0.06dur # to follow the linar scheduler with cooldown recipe
  alpha_f: 0.01

optimizer:
  name: decoupled_adamw
  lr: 5.0e-4 # Peak learning rate
  betas:
  - 0.9
  - 0.96 # to follow the linar scheduler with cooldown recipe
  eps: 1.0e-06
  weight_decay: 5.0e-5 # to follow the linar scheduler with cooldown recipe

algorithms:
  low_precision_layernorm: {}

max_duration: 20ep # Subsample the training data for ~275M samples
eval_interval: 1ep
global_train_batch_size: 4096

# System
seed: 17
device_eval_batch_size: 256
device_train_microbatch_size: 128 
# device_train_microbatch_size: auto
precision: amp_bf16

# Logging
progress_bar: true
log_to_console: true
console_log_interval: 100ba

callbacks:
  speed_monitor:
    window_size: 500
  lr_monitor: {}

# (Optional) W&B logging
loggers:
  wandb:
    project: llm-foundry-scripts_train-zehua-bert      # Fill this in

# (Optional) Checkpoint to local filesystem or remote object store
save_interval: 2000ba
save_num_checkpoints_to_keep: 10  # Important, this cleans up checkpoints saved to DISK
save_folder: ./{run_name}/checkpoints

# (Optional) Load from local filesystem or remote object store to
# start from an existing model checkpoint;
# e.g. './ckpt/latest-rank{rank}.pt' (local), or
# 's3://mybucket/mydir/ckpt/latest-rank{rank}.pt' (remote)
# load_path: ./mosaic-bert-base-uncased/checkpoints/latest-rank{rank}.pt